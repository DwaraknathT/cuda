cuda cheat-sheet

cudaMemcpy - copies memory from host ot device-args, destination, source, size,
cudaMemcpyHostToDevice)
ex = cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice)

cudaMalloc - allocates memeory in the device, args - address of variable name, size
note- the variable address must be cast into pointer of pointer of void
ex - cudaMalloc((void**) &d_A, size)

cudaFree() - frees object from device global memory, args - pointer to freed object
Kernel fucntions - funcs that run on the device -uses the single program multiple data
parallel paradigm

ex- simple vector add kernel
//the kernel

__global__
void vecAddKernel(float *A, float*B, float *c, int n)
{
	int i = blockDim.x*blockIdx.x + threadIdx.x
	if (i <c ) C[i] = A[i] + B[i]
}

void vecAdd ( float* A, float* B, float* C, int n)
{
	//initialize and move data from host to device
	float *d_A, *d_B, *d_C;
	int size = n * sizeof(float);
	cudaMalloc((void**) &d_A, size);
	cudaMalloc((void**) &d_B, size);
	cudaMemcpy(d_A, A, size, cudaMemcpyHosttoDevice);
	cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

	//launch the kernel to add elements
	vecAddKernel <<<ceil(n/256.0), 256>>> (d_A, d_B, d_C, n);
	//copy the answer back to host memory
	cudaMemcpy( C, d_C, size, cudaMemcpyDeviceToHost)

	//free the memory
	cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
}

All cuda threads in a grid executes the same kernel function.
specify the griddim and block dim using variables of size dim3, dim3 is a c struct
which has 3 vales

ex - dim3 dimGrid (32,1,1)
     dim3 dimBlock (128, 1, 1)
     vecAddkernel<<<dimGrid, dimBlock>>>(..)

shorthand notation - instead of mentioning all dims mention only a scalar in kernel launch
fucntion
ex - vecAdddKernel<<< (n/256), 256>>> x dimension is initialized and y and z are automatically
set to 1.

gridDIm.x - range = (1 to 65536)
total number of threads in a block (upper limit) - 1024
choice of dim of thread depends on nature of data
ex- processing an input image d_pin, output image = d_pout, kernel dimensions = 2x2
m = total no of threads in x direction, n = total no of threads in y direction

dim3 dimGrid(ceil(n/16.0), ceil(m/16.0), 1);
dim3 dimBlock(16, 16, 1);
colorToGreyConversion<<<dimGrid, dimBlock>>>(d_pin, d_pout);
 q) write cuda program to convert an image from rgb to grayscale
formula l = r*0.21 + g*0.72 + b*0.07

__global__ void colorToGrayConversion(unsigned char*pout, unsigned char* pin, int width, int hieght)
{
	int Row = threadIdx.y + blockDim.y*blockIdx.y
	int Col = threadIdx.x + blockDim.x*blockIdx.x
	if (Col < width && Row < height)
	{
		int grayOffset 	= Row*width + Col;
		int rgbOffset = grayOffset*Channels;
		unsigned char r = P_in[rgbOffset];
		unsigned char g = p_in[rgbOffset + 2];
		unsigned char b = p_in[rgbOffset + 3];
		pout[grayOffset] = //formula
	}
}

q) write a kernel for image smoothening

__global__ void ImageSmoothening (unsigned char *in, unsigned char *out,
																	int w, int h)
{
	int Col = blockDim.x*blockIdx.x + threadIdx.x;
	int Row = blockDim.y*blockIdx.y + threadIdx.y;
	if ( Col<h && Row<w)
	{
		int pixVal = 0
		int pixels = 0
		//get the average of surrounding pixels
		for (int blurRow = -BLUR_SIZE, blurRow< BLUR_SIZE+1, blurRow++)
		{
			for (int blurCol = -BLUE_COL, blurCol<BLUR_SIZE+1, blurCol++)
			{
				int curRow = Row + blurRow;
				int curCol = Col + blurCol;
				if (curRow > -1 && curRow < w && curCol > -1 && curCol < h)
				{
					pixelVal += in[curRow*w + curCol];
					pixels++;
				}
			}
		}
	}
}

CUDA allows threads in same block to coordinate by using barrier synchronzation
function __syncthreads() - when this is called, it will be held in the calling
location unstil every thread reaches that location.
This ensures that all hreads finish a phase first.
Transparent scalability - Ability to execute the same application program on
hardware with differnet numbers of execution resources

Resource Assignment -
Execution resources are organized into Streaming Multiprocesssors
Query device properties - How the hell do we know how many SM are there ?
int dev_count; //number of cuda devices
cudaGetDeviceCount(&dev_Count); //returns the number of available CUDA devices
Iterate through the devices and get their properties

cudaDeviceProp dev_prop;   // c- structure
for ( int i=0; i< dev_count; ++i )
{
	cudaGetDeviceProperties (&dev_prop, i);//decide is device has enough reosurces
}
properties of device:
dev_prop.maxThreadsPerBLock; // indicates max threads per block
dev_prop.MultiProcessorCount // number of SM
dev_prop.clockRate //clock speed of the device

Thread scheduling and latency tolerance:
Thread shceduling must be done in specific context of hardware
warps - blocks in SM are divided into 32 thread units
// probably why threads are always multiples of 32
Latency tolerance - Filling the gaps in execution(when one warp is waiting for
data from another warp) with execution of execution ready warps
zero-overhead thread scheduliong - 0 wastage of execution time

--------------------------------------------------------------------------------------
chap 3 - Memory and data locality

Importance of reducing global memory calls - 

While having numerous threads available for execution can theoretically tolerate long memory
access latencies, one can easily run into a situation where traffic congestion in the global
memory access paths prevents all but very few threads from making progress, thus rendering 
some of the Streaming Multiprocessors (SMs) idle.

for example in the image blurring kernel's inside for loop, each execution one global access 
is performed for one flaoting-point addition. 
compute to global memory access ratio - ratio of floating point calculations to global memory 
access operations. 
memory bound programs- Programs whose execution speed is limited by memory access throughput 

Matrix multiplication - let M and N be 2 sqyare matrices, M and N are linearized in memory 
Mij = i*width + j

__global__ void MatrixMulKernel (float *M, float *N, float *P, int width) 
{
	//calculate the row index of P element and M 
	int row = blockIdx.y*blockDim.y + threadIdx.y;
	//calcualte col index of P and N 
	int col = blockIdx.x*blockDim.x + threadIdx.x;
	
	if((row<width)&&(col<width))
	{
		floaat pvalue =0;
		for (int i =0;i<width;++i)
		{
			pvalue+=M[row*width +k]*N[k*width+col];
		}
		P[row*width + col]= pvalue;
	}
}
Note : in every inner production calculation two global memory access are performed for one 
floating point multiplication and one floating point addition.

CUDA memory types: CUDA device contains several types of memory, Global memory and constant 
memory can be written and read by the host calling API fucntions. gloabal memory can be 
written to and read by the device, constant mem supports high bandwidth short latency read
only access by the device. Registers and shared memory are on-chip memories. Variables that 
reside in these types of memory can be accessed at very high-speed in a highly parallel 
manner. Registers are allocated to individual threads; each thread can only access its own 
registers.
shared memory isdesigned to support, efficient high bandwidth sharing of data among threads 
in a block. 

different types of variable declaration 
__device__ __shared__ int sharedValue; shared memory 
__device__ int globalVar; device global memory 
__device__ __constant__ int constVar; device Constant memory  
scalar varialbes - variables that are not matrices or vectors, all automatic scalar variables 
declared in kernel and device functions are placed into registers 

Tiling for reduced memory traffic:
The shared memory is really fast, but also small. To fit entire data into the share memory 
we divide the entire memory block into tiles such that each tile fits into the sahred mem. 
imp point: kernel computations performed on these tiles must be independent of each other 
To reduce the memory overhead we can combine memory access of multiple thread into one huge 
memory access request, so we dont have to access slow global memory again and again.

condition: the memory request of different threads must be in the same temporal proximity

Tiled Matrix multiplication 

__global__ void TiledMatrixMultiplication (float *d_M, float *d_N, float *d_P, int Width)
{
	__shared__ float Mds[tile_width][tile_width];
	__shared__ float Nds[tile_width][tile_width];
	int bx = blockIdx.x; int by = blockIdx.y;
	int tx = threadIdx.x; int ty = threadIdx.y;
	//Identiy row and col of d_p element 
	int row = by*tile_width + ty;
	int col = bx*tile_width + tx;

	float pvalue =0;
	for (int ph=0;ph<width/tile_width; ++ph)
	{
		//collaborative loading of d_m and d_n 
		Mds[ty][tx]=d_M[row*width+ph*width+tx0];
		Nds[ty][tx]=d_N[(ph*tile_width+ty)*width + col];
		__syncthreads();
		
		for (int k=0;k<tile_width;++k)
		{
			pvalue+= Mds[ty][k]*Nds[k][tx];
		}
		__syncthreads();
	}
	d_p[row*width+col] = pvalue;
}

--------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------
Chapter 5 - Performance Considerations 

Current CUDA devices employ a technique that allows programmers to achieve high global memory 
access of threads into favorable patterns, by taking advantage of the fact that threads in 
a warp execute the same instruction at any given point. the most favorable access pattern is
 achieved when all threads in a warp access consecutive global memory locations. In this 
case, the hardware combines, or coalesces, all these accesses into a consolidated access to 
consecutive DRAM locations.
If an algorithm intrinsically requires a kernel code to iterate through data along the row direction,
one can use the shared memory to enable memory coalescing. The technique, called corner 
turning.  Once the data is in shared memory, they can be accessed either on a row basis or 
a column basis with much less performance variation because the shared memories are 
implemented as intrinsically high-speed on-chip memory that does not require coalescing to 
achieve high data access rate. 

DRAM systems typically employ two more forms of parallel organization – banks and channels.
At the highest level, a processor contains one or more channels. Each channel is a memory
controller with a bus that connects a set of DRAM banks to the processor. 

Warps and SIMD hardware 
Every warp - 32 threads 

Dynamic partitioning of resources 
The execution resources in an SM include registers, shared memory, thread block slots, and
thread slots. 
 “performance cliff” where a slight increase in resource usage can result in significant 
reduction in parallelism and performance achieved, reduced thread parallelism can negatively 
affect the utilization of the memory access bandwidth of the DRAM system. The reduced memory 
access throughput, in turn, can further reduce the thread execution throughput. This is a 
pitfall that can result in disappointing performance of tiled algorithms and should be 
carefully avoided.

Thread Granularity 
It is sometimes advantageous to put more work into each thread and use fewer threads. Such 
advantage arises when some redundant work exists between threads. In the current generation 
of devices, each SM has limited instruction processing bandwidth. Every instruction consumes
instruction processing bandwidth, whether it is a floating-point calculation instruction, a 
load instruction, or a branch instruction. Eliminating redundant work can ease the pressure 
on the instruction processing bandwidth and improve the overall execution speed of the 
kernel. 

--------------------------------------------------------------------------------------------

Chapter 6 - Numerical Consideratons 

A floating-point number system starts with the representation of a numerical value as bit 
patterns. In the IEEE Floating-Point Standard, a numerical value is represented in three 
groups of bits: sign (S), exponent (E), and mantissa (M). 